{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence correction gpt on Korean\n",
    "\n",
    "한국어 문법교정 GPT3 모델 구축하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.12.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (217 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.1.post1-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.1-cp312-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.12/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in ./lib/python3.12/site-packages (from accelerate) (5.9.8)\n",
      "Collecting pyyaml (from accelerate)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub (from accelerate)\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate)\n",
      "  Using cached safetensors-0.4.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./lib/python3.12/site-packages (from ipywidgets) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./lib/python3.12/site-packages (from ipywidgets) (5.14.2)\n",
      "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in ./lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in ./lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Using cached pandas-2.2.1-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached scipy-1.12.0-cp312-cp312-macosx_12_0_arm64.whl (31.4 MB)\n",
      "Using cached scikit_learn-1.4.1.post1-cp312-cp312-macosx_12_0_arm64.whl (10.5 MB)\n",
      "Using cached torch-2.2.1-cp312-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Using cached accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "Using cached transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
      "Using cached ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl (292 kB)\n",
      "Using cached safetensors-0.4.2-cp312-cp312-macosx_11_0_arm64.whl (393 kB)\n",
      "Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached tokenizers-0.15.2-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pytz, mpmath, widgetsnbextension, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, numpy, networkx, MarkupSafe, jupyterlab-widgets, joblib, idna, fsspec, filelock, charset-normalizer, certifi, scipy, requests, pandas, jinja2, torch, scikit-learn, huggingface-hub, tokenizers, ipywidgets, accelerate, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 accelerate-0.28.0 certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.3.1 huggingface-hub-0.21.4 idna-3.6 ipywidgets-8.1.2 jinja2-3.1.3 joblib-1.3.2 jupyterlab-widgets-3.0.10 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.4 pandas-2.2.1 pytz-2024.1 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 sympy-1.12 threadpoolctl-3.4.0 tokenizers-0.15.2 torch-2.2.1 tqdm-4.66.2 transformers-4.39.0 typing-extensions-4.10.0 tzdata-2024.1 urllib3-2.2.1 widgetsnbextension-4.0.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scipy scikit-learn torch accelerate transformers ipywidgets tqdm\n",
    "\n",
    "# Transformers 를 사용하기 위한 라이브러리들 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-Hub data 정제\n",
    "\n",
    "예시 데이터는 다음과 같다.\n",
    "\n",
    "이 프로젝트를 수행하는 데이터인 AI-Hub 데이터는 용량이 112GB, 국외반출 불가로 데이터 관리에 주의를 요한다.\n",
    "\n",
    "이 데이터를 Pandas.DataFrame['index', text', 'corrected'] 로 정제하는 과정을 거친다.\n",
    "\n",
    "```json\n",
    "{  \"id\": \"100008-1-1-1\",\n",
    "  \"fileName\": \"TX_CA_1_100008-1-1-1\",\n",
    "  \"dataSet\": \"한국어 철자 및 맞춤법 교정용 병렬 데이터\",\n",
    "  \"domain\": \"CA\",\n",
    "  \"ko\": \"지금까지 다녀 본 여행지 중 좋았던 곳 추천해줘.\",\n",
    "  \"corrected\": \"지금까지 다녀 본 여행지 중 좋았던 곳 추천해 줘.\",\n",
    "  \"error\": [\n",
    "    {\n",
    "      \"errorType\": \"spac\",\n",
    "      \"startPoint\": 22,\n",
    "      \"endPoint\": 27\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "LOCATION = '.'\n",
    "\n",
    "def read_json_file(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    JSON 파일을 읽어와 파싱한 데이터를 리스트로 반환하는 함수\n",
    "    :param file_path: 읽어올 JSON 파일의 경로\n",
    "    :return: JSON 파일을 파싱한 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=['text', 'corrected'])\n",
    "\n",
    "    # JSON 파일 읽어오기\n",
    "    with open(file_path, 'r') as f:\n",
    "        data: List[Dict] = json.load(f)\n",
    "\n",
    "        for p in tqdm(data):\n",
    "            df.loc[len(df)] = {'text': p.get('ko'), 'corrected': p.get('corrected')}\n",
    "\n",
    "train = [\n",
    "\n",
    "]\n",
    "validate = [\n",
    "\n",
    "]\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "validate_data = pd.DataFrame()\n",
    "\n",
    "for file in tqdm(train):\n",
    "    train_data = pd.concat([train_data, read_json_file(f'{LOCATION}/{file}')], ignore_index=True)\n",
    "for file in tqdm(validate):\n",
    "    validate_data = pd.concat([validate_data, read_json_file(f'{LOCATION}/{file}')], ignore_index=True)\n",
    "\n",
    "train_data.to_csv(f'{LOCATION}/train.csv')\n",
    "validate_data.to_csv(f'{LOCATION}/validate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 학습 준비\n",
    "\n",
    "학습 준비된 데이터는 토크나이저를 불러와 DataLoader에 준비하여 학습 준비를 한다.\n",
    "\n",
    "파인튜닝 대상 모델은 다음과 같다.\n",
    "[kykim/gpt3-kor-small_based_on_gpt2](https://github.com/kiyoungkim1/LMkor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "LOCATION = '.'\n",
    "\n",
    "# GPT-3 모델과 토크나이저 불러오기\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('kykim/gpt3-kor-small_based_on_gpt2')\n",
    "max_length = 100\n",
    "loader_size = 16\n",
    "\n",
    "# 훈련용 및 검증용 데이터셋 클래스 정의\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.data.iloc[idx]['text']\n",
    "        output_text = self.data.iloc[idx]['corrected']\n",
    "        input_ids = self.tokenizer.encode(input_text, max_length=self.max_length, truncation=True, padding='max_length')\n",
    "        output_ids = self.tokenizer.encode(output_text, max_length=self.max_length, truncation=True, padding='max_length')\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'output_ids': torch.tensor(output_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train = pd.read_csv(f'{LOCATION}/train.csv')\n",
    "validate = pd.read_csv(f'{LOCATION}/validate.csv')\n",
    "\n",
    "train_dataset = TextDataset(train, tokenizer, max_length)\n",
    "validate_dataset = TextDataset(validate, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=loader_size, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=loader_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 모델 및 옵티마이저 불러오기\n",
    "model = GPT2LMHeadModel.from_pretrained('kykim/gpt3-kor-small_based_on_gpt2')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 및 검증 함수 정의\n",
    "def train(train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        inputs = batch['input_ids'].to(model.device)\n",
    "        labels = batch['output_ids'].to(model.device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(validate_loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validate_loader):\n",
    "            inputs = batch['input_ids'].to(model.device)\n",
    "            labels = batch['output_ids'].to(model.device)\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(validate_loader)\n",
    "\n",
    "# 학습 및 검증 수행\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train(train_loader, model, optimizer)\n",
    "    validate_loss = validate(validate_loader, model)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Validate Loss: {validate_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentence_correction_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
